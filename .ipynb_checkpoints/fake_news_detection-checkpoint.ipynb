{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class to read the dataset\n",
    "class Dataset():\n",
    "  def __init__(self, train_stance, test_stance, train_body, test_body):\n",
    "    self.train_stance = train_stance\n",
    "    self.test_stance = test_stance\n",
    "    self.train_body = train_body\n",
    "    self.test_body = test_body\n",
    "\n",
    "    print(\"Dataset length:\")\n",
    "\n",
    "    self.train_stances = self.read_stance(self.train_stance)\n",
    "    self.test_stances = self.read_stance(self.test_stance)\n",
    "    self.train_bodies = self.read_body(self.train_body)\n",
    "    self.test_bodies = self.read_body(self.test_body)\n",
    "\n",
    "    print(\"Total train stances: \" + str(len(self.train_stances)))\n",
    "    print(\"Total test stances: \" + str(len(self.test_stances)))\n",
    "    print(\"Total train bodies: \" + str(len(self.train_bodies)))\n",
    "    print(\"Total test bodies: \" + str(len(self.test_bodies)))\n",
    "\n",
    "  def read_stance(self, path):\n",
    "    rows = []\n",
    "    with open(path, encoding='utf-8', errors='ignore') as csvfile:\n",
    "      r = csv.DictReader(csvfile)\n",
    "      for row in r:\n",
    "        rows.append([row['Body ID'], row['Headline'], row['Stance']])\n",
    "    return rows\n",
    "\n",
    "  def read_body(self, path):\n",
    "    rows = []\n",
    "    #with open(path, encoding='utf-8') as csvfile:\n",
    "    with open(path, encoding=\"utf8\", errors='ignore') as csvfile:\n",
    "      r = csv.DictReader(csvfile)\n",
    "      for row in r:\n",
    "        rows.append([row['Body ID'], row['articleBody']])\n",
    "        #rows[row['Body ID']] = row['articleBody']\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:\n",
      "Total train stances: 49972\n",
      "Total test stances: 25413\n",
      "Total train bodies: 1683\n",
      "Total test bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = Dataset('./fnc1/train_stances.csv', './fnc1/competition_test_stances.csv', './fnc1/train_bodies.csv', './fnc1/competition_test_bodies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2008',\n",
       " 'Ferguson riots: Pregnant woman loses eye after cops fire BEAN BAG round through car window',\n",
       " 'unrelated']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at datatset\n",
    "data.train_stances[0]\n",
    "data.test_stances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'Al-Sisi has denied Israeli reports stating that he offered to extend the Gaza Strip.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_bodies[0]\n",
    "data.test_bodies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process function (lowercase, stopwords, lemmatization)\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(processed_dataset):\n",
    "  for w in processed_dataset:\n",
    "    words = []\n",
    "    w[1] = word_tokenize(w[1])\n",
    "    for token in w[1]:\n",
    "      lower = token.lower()\n",
    "      if lower not in stopwords and lower.isalpha():\n",
    "        word = porter.stem(lower)\n",
    "        words.append(word)\n",
    "    w[1] = words\n",
    "  return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call pre-process functions for train and test\n",
    "train_headline = preprocess(data.train_stances)\n",
    "test_headline = preprocess(data.test_stances)\n",
    "train_content = preprocess(data.train_bodies)\n",
    "test_content = preprocess(data.test_bodies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', ['soldier', 'shot', 'parliament', 'lock', 'gunfir', 'erupt', 'war', 'memori'], 'unrelated']\n",
      "['2008', ['ferguson', 'riot', 'pregnant', 'woman', 'lose', 'eye', 'cop', 'fire', 'bean', 'bag', 'round', 'car', 'window'], 'unrelated']\n",
      "['0', ['small', 'meteorit', 'crash', 'wood', 'area', 'nicaragua', 'capit', 'managua', 'overnight', 'govern', 'said', 'sunday', 'resid', 'report', 'hear', 'mysteri', 'boom', 'left', 'deep', 'crater', 'near', 'citi', 'airport', 'associ', 'press', 'report', 'govern', 'spokeswoman', 'rosario', 'murillo', 'said', 'committe', 'form', 'govern', 'studi', 'event', 'determin', 'rel', 'small', 'meteorit', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'asteroid', 'rc', 'measur', 'feet', 'diamet', 'skim', 'earth', 'weekend', 'abc', 'news', 'report', 'murillo', 'said', 'nicaragua', 'ask', 'intern', 'expert', 'help', 'local', 'scientist', 'understand', 'happen', 'crater', 'left', 'meteorit', 'radiu', 'feet', 'depth', 'feet', 'said', 'humberto', 'saballo', 'volcanologist', 'nicaraguan', 'institut', 'territori', 'studi', 'committe', 'said', 'still', 'clear', 'meteorit', 'disintegr', 'buri', 'humberto', 'garcia', 'astronomi', 'center', 'nation', 'autonom', 'univers', 'nicaragua', 'said', 'meteorit', 'could', 'relat', 'asteroid', 'forecast', 'pass', 'planet', 'saturday', 'night', 'studi', 'could', 'ice', 'rock', 'said', 'wilfri', 'strauch', 'advis', 'institut', 'territori', 'studi', 'said', 'strang', 'one', 'report', 'streak', 'light', 'ask', 'anyon', 'photo', 'someth', 'local', 'resid', 'report', 'hear', 'loud', 'boom', 'saturday', 'night', 'said', 'see', 'anyth', 'strang', 'sky', 'sit', 'porch', 'saw', 'noth', 'sudden', 'heard', 'larg', 'blast', 'thought', 'bomb', 'felt', 'expans', 'wave', 'jorg', 'santamaria', 'told', 'associ', 'press', 'site', 'crater', 'near', 'managua', 'intern', 'airport', 'air', 'forc', 'base', 'journalist', 'state', 'media', 'allow', 'visit']]\n",
      "['1', ['deni', 'isra', 'report', 'state', 'offer', 'extend', 'gaza', 'strip']]\n"
     ]
    }
   ],
   "source": [
    "print(train_headline[0])\n",
    "print(test_headline[0])\n",
    "print(train_content[0])\n",
    "print(test_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all sentences into one collection for word2vec training.\n",
    "# It takes word tokenized sentences, which looks like [\"hello\", \"world\", ... ]\n",
    "\n",
    "sent_collection = []\n",
    "\n",
    "def sent_list(s_list, t_list):\n",
    "  for sent in s_list:\n",
    "    t_list.append(sent[1])\n",
    "  return\n",
    "\n",
    "sent_list(train_headline, sent_collection)\n",
    "sent_list(test_headline, sent_collection)\n",
    "sent_list(train_content, sent_collection)\n",
    "sent_list(test_content, sent_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soldier', 'shot', 'parliament', 'lock', 'gunfir', 'erupt', 'war', 'memori']\n"
     ]
    }
   ],
   "source": [
    "print(sent_collection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084\n"
     ]
    }
   ],
   "source": [
    "# to see how many unique words in the collection\n",
    "bag = []\n",
    "for s in sent_collection:\n",
    "  for w in s:\n",
    "    bag.append(w)\n",
    "\n",
    "p = set(bag)\n",
    "print(len(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the word2vec with customized words because the stemming makes some words not recognizable, such as \"polic\" and \"strang\"\n",
    "# details  https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "#import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                    window=2,\n",
    "                    size=100,\n",
    "                    sample=6e-5, \n",
    "                    alpha=0.03, \n",
    "                    min_alpha=0.0007, \n",
    "                    negative=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sent_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17339019, 33276930)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sent_collection, total_examples=w2v_model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes the memory more efficient since we do not plan tot train any further\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18084"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03170666,  0.05994361, -0.01594953, -0.01158441, -0.2553048 ,\n",
       "        0.15896626, -0.05707419,  0.06622464,  0.1167105 ,  0.02071714,\n",
       "        0.01473159,  0.08760121,  0.02629115,  0.06877466,  0.14730984,\n",
       "        0.00831118, -0.00957257,  0.0364738 ,  0.01016295, -0.00858984,\n",
       "        0.01968961, -0.12182599, -0.00251548, -0.10242035,  0.01302129,\n",
       "       -0.03282655, -0.11320949,  0.11239958,  0.10937969, -0.10672441,\n",
       "       -0.00179643, -0.04826476,  0.01923433,  0.18598905,  0.15126215,\n",
       "       -0.17327411, -0.02258573, -0.0134278 ,  0.0591633 ,  0.15780906,\n",
       "        0.09686403, -0.03460663, -0.20300788, -0.06884312,  0.10651762,\n",
       "       -0.02109622,  0.09848445,  0.09285858, -0.01213906, -0.0651632 ,\n",
       "        0.10628242, -0.02083327, -0.03462162, -0.08759736,  0.21147734,\n",
       "        0.04124995,  0.00487584,  0.03875668,  0.0316501 ,  0.00573582,\n",
       "        0.13343246,  0.10616208,  0.11232797, -0.03080473, -0.1765491 ,\n",
       "        0.01725558, -0.21733336,  0.0200403 ,  0.1301482 , -0.19204529,\n",
       "       -0.0253341 ,  0.03048412, -0.157898  , -0.10348459, -0.11907003,\n",
       "       -0.1147286 , -0.05432421, -0.07850005,  0.13270453, -0.04214988,\n",
       "       -0.04537453, -0.01052979,  0.01296469, -0.08103745,  0.21435371,\n",
       "        0.02875281, -0.09801527, -0.18954329,  0.1973032 ,  0.13804442,\n",
       "       -0.02303872, -0.09735751, -0.02806089, -0.00513703, -0.09414748,\n",
       "       -0.07477094,  0.04246349, -0.05472517, -0.08868736, -0.15728256],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector('polic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053465083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('polic', 'strang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine headline and body\n",
    "import nltk\n",
    "def comb_list (stance, body, target):\n",
    "  for i in body:\n",
    "    for j in stance:\n",
    "      if j[0] == i[0]:\n",
    "        i[1] = nltk.FreqDist(i[1])\n",
    "        target.append([j[0], j[1], i[1], j[2]])\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "comb_list(train_headline, train_content, train_set)\n",
    "comb_list(test_headline, test_content, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25413\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
