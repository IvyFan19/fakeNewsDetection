{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class to read the dataset\n",
    "class Dataset():\n",
    "  def __init__(self, train_stance, test_stance, train_body, test_body):\n",
    "    self.train_stance = train_stance\n",
    "    self.test_stance = test_stance\n",
    "    self.train_body = train_body\n",
    "    self.test_body = test_body\n",
    "\n",
    "    print(\"Dataset length:\")\n",
    "\n",
    "    self.train_stances = self.read_stance(self.train_stance)\n",
    "    self.test_stances = self.read_stance(self.test_stance)\n",
    "    self.train_bodies = self.read_body(self.train_body)\n",
    "    self.test_bodies = self.read_body(self.test_body)\n",
    "\n",
    "    print(\"Total train stances: \" + str(len(self.train_stances)))\n",
    "    print(\"Total test stances: \" + str(len(self.test_stances)))\n",
    "    print(\"Total train bodies: \" + str(len(self.train_bodies)))\n",
    "    print(\"Total test bodies: \" + str(len(self.test_bodies)))\n",
    "\n",
    "  def read_stance(self, path):\n",
    "    rows = []\n",
    "    with open(path, encoding='utf-8', errors='ignore') as csvfile:\n",
    "      r = csv.DictReader(csvfile)\n",
    "      for row in r:\n",
    "        rows.append([row['Body ID'], row['Headline'], row['Stance']])\n",
    "    return rows\n",
    "\n",
    "  def read_body(self, path):\n",
    "    rows = []\n",
    "    #with open(path, encoding='utf-8') as csvfile:\n",
    "    with open(path, encoding=\"utf8\", errors='ignore') as csvfile:\n",
    "      r = csv.DictReader(csvfile)\n",
    "      for row in r:\n",
    "        rows.append([row['Body ID'], row['articleBody']])\n",
    "        #rows[row['Body ID']] = row['articleBody']\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:\n",
      "Total train stances: 49972\n",
      "Total test stances: 25413\n",
      "Total train bodies: 1683\n",
      "Total test bodies: 904\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = Dataset('./fnc1/train_stances.csv', './fnc1/competition_test_stances.csv', './fnc1/train_bodies.csv', './fnc1/competition_test_bodies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(body#, headline, result): ['0', 'Soldier shot, Parliament locked down after gunfire erupts at war memorial', 'unrelated']\n",
      "(body#, headline, result): ['2008', 'Ferguson riots: Pregnant woman loses eye after cops fire BEAN BAG round through car window', 'unrelated']\n"
     ]
    }
   ],
   "source": [
    "print(\"(body#, headline, result):\", data.train_stances[0])\n",
    "print(\"(body#, headline, result):\", data.test_stances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(body#, article): ['0', 'A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.']\n",
      "(body#, article): ['1', 'Al-Sisi has denied Israeli reports stating that he offered to extend the Gaza Strip.']\n"
     ]
    }
   ],
   "source": [
    "print(\"(body#, article):\", data.train_bodies[0])\n",
    "print(\"(body#, article):\", data.test_bodies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process function (lowercase, stopwords, lemmatization)\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(processed_dataset):\n",
    "  for w in processed_dataset:\n",
    "    words = []\n",
    "    w[1] = word_tokenize(w[1])\n",
    "    for token in w[1]:\n",
    "      lower = token.lower()\n",
    "      if lower not in stopwords and lower.isalpha():\n",
    "        word = porter.stem(lower)\n",
    "        words.append(word)\n",
    "    w[1] = words\n",
    "  return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call pre-process functions for train and test\n",
    "train_headline = preprocess(data.train_stances)\n",
    "test_headline = preprocess(data.test_stances)\n",
    "train_content = preprocess(data.train_bodies)\n",
    "test_content = preprocess(data.test_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', ['soldier', 'shot', 'parliament', 'lock', 'gunfir', 'erupt', 'war', 'memori'], 'unrelated']\n",
      "['2008', ['ferguson', 'riot', 'pregnant', 'woman', 'lose', 'eye', 'cop', 'fire', 'bean', 'bag', 'round', 'car', 'window'], 'unrelated']\n",
      "['0', ['small', 'meteorit', 'crash', 'wood', 'area', 'nicaragua', 'capit', 'managua', 'overnight', 'govern', 'said', 'sunday', 'resid', 'report', 'hear', 'mysteri', 'boom', 'left', 'deep', 'crater', 'near', 'citi', 'airport', 'associ', 'press', 'report', 'govern', 'spokeswoman', 'rosario', 'murillo', 'said', 'committe', 'form', 'govern', 'studi', 'event', 'determin', 'rel', 'small', 'meteorit', 'appear', 'come', 'asteroid', 'pass', 'close', 'earth', 'asteroid', 'rc', 'measur', 'feet', 'diamet', 'skim', 'earth', 'weekend', 'abc', 'news', 'report', 'murillo', 'said', 'nicaragua', 'ask', 'intern', 'expert', 'help', 'local', 'scientist', 'understand', 'happen', 'crater', 'left', 'meteorit', 'radiu', 'feet', 'depth', 'feet', 'said', 'humberto', 'saballo', 'volcanologist', 'nicaraguan', 'institut', 'territori', 'studi', 'committe', 'said', 'still', 'clear', 'meteorit', 'disintegr', 'buri', 'humberto', 'garcia', 'astronomi', 'center', 'nation', 'autonom', 'univers', 'nicaragua', 'said', 'meteorit', 'could', 'relat', 'asteroid', 'forecast', 'pass', 'planet', 'saturday', 'night', 'studi', 'could', 'ice', 'rock', 'said', 'wilfri', 'strauch', 'advis', 'institut', 'territori', 'studi', 'said', 'strang', 'one', 'report', 'streak', 'light', 'ask', 'anyon', 'photo', 'someth', 'local', 'resid', 'report', 'hear', 'loud', 'boom', 'saturday', 'night', 'said', 'see', 'anyth', 'strang', 'sky', 'sit', 'porch', 'saw', 'noth', 'sudden', 'heard', 'larg', 'blast', 'thought', 'bomb', 'felt', 'expans', 'wave', 'jorg', 'santamaria', 'told', 'associ', 'press', 'site', 'crater', 'near', 'managua', 'intern', 'airport', 'air', 'forc', 'base', 'journalist', 'state', 'media', 'allow', 'visit']]\n",
      "['1', ['deni', 'isra', 'report', 'state', 'offer', 'extend', 'gaza', 'strip']]\n"
     ]
    }
   ],
   "source": [
    "print(train_headline[0])\n",
    "print(test_headline[0])\n",
    "print(train_content[0])\n",
    "print(test_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all sentences into one collection for word2vec training.\n",
    "# It takes word tokenized sentences, which looks like [\"hello\", \"world\", ... ]\n",
    "\n",
    "sent_collection = []\n",
    "\n",
    "def sent_list(s_list, t_list):\n",
    "  for sent in s_list:\n",
    "    t_list.append(sent[1])\n",
    "  return\n",
    "\n",
    "sent_list(train_headline, sent_collection)\n",
    "sent_list(test_headline, sent_collection)\n",
    "sent_list(train_content, sent_collection)\n",
    "sent_list(test_content, sent_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soldier', 'shot', 'parliament', 'lock', 'gunfir', 'erupt', 'war', 'memori']\n"
     ]
    }
   ],
   "source": [
    "print(sent_collection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18084\n"
     ]
    }
   ],
   "source": [
    "# to see how many unique words in the collection\n",
    "bag = []\n",
    "for s in sent_collection:\n",
    "  for w in s:\n",
    "    bag.append(w)\n",
    "\n",
    "p = set(bag)\n",
    "print(len(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d4800b6ad5f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#import multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#cores = multiprocessing.cpu_count()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# train the word2vec with customized words because the stemming makes some words not recognizable, such as \"polic\" and \"strang\"\n",
    "# details  https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "#import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                    window=2,\n",
    "                    size=100,\n",
    "                    sample=6e-5, \n",
    "                    alpha=0.03, \n",
    "                    min_alpha=0.0007, \n",
    "                    negative=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sent_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(sent_collection, total_examples=w2v_model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes the memory more efficient since we do not plan tot train any further\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.get_vector('polic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('polic', 'strang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine headline and body\n",
    "import nltk\n",
    "def comb_list (stance, body, target):\n",
    "  for i in body:\n",
    "    for j in stance:\n",
    "      if j[0] == i[0]:\n",
    "        i[1] = nltk.FreqDist(i[1])\n",
    "        target.append([j[0], j[1], i[1], j[2]])\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "comb_list(train_headline, train_content, train_set)\n",
    "comb_list(test_headline, test_content, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
